{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/elizabethavargas/Llama-Fine-tuning/blob/main/curr_Contest_DL_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jngXVlfmqT9A"
   },
   "source": [
    "\n",
    "\n",
    " <h1>\n",
    "Welcome to the Math Question Answer Verification Competition! üöÄ\n",
    "\n",
    "The goal is to fine-tune a Llama-3-8B model to predict if a given solution to a math problem is correct or not. Your model should output True if the solution is correct, and False otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6H4hQVSqblY"
   },
   "source": [
    "## **Step 1: Install Necessary Libraries**\n",
    "\n",
    "First, we need to install the required Python libraries. We'll be using the unsloth library, which provides highly efficient, memory-saving training methods for large language models, making it possible to fine-tune powerful models on a single free-tier GPU. We'll also install xformers for further optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git@31b667b54139962832ea2de890383eed14a0a17d\"\n",
    "# !pip install --no-deps \"xformers<0.0.26\" \"trl<0.9.0\" \"peft<0.12.0\" \"accelerate<0.32.0\" \"bitsandbytes<0.44.0\" \"transformers<4.43.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkuYDaVuravN"
   },
   "source": [
    "## **Step 2: Load the Model and Tokenizer**\n",
    "\n",
    "Next, we'll load the Llama-3-8B model, which is the only model permitted for this competition. We'll use Unsloth's FastLanguageModel to handle this efficiently.\n",
    "\n",
    "A key technique we'll use is 4-bit quantization (load_in_4bit = True). Think of this as compressing the model's knowledge into a much smaller file size. This significantly reduces the amount of GPU memory required, allowing us to fine-tune this large model even on a free platform like Google Colab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 4096  # Choose any sequence length\n",
    "dtype = None  # This will auto-detect the best data type for your GPU\n",
    "load_in_4bit = True  # Use 4-bit quantization to save memory\n",
    "\n",
    "checkpoint_path = f\"/content/drive/MyDrive/unsloth_math_model\"\n",
    "\n",
    "print(f\"Loading model from: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "# Load the model and tokenizer from Hugging Face\n",
    "# Note: We use the base model, not a 4-bit pre-quantized one,\n",
    "# to ensure we start from the official weights.\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name =  checkpoint_path, #\"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "tokenizer.chat_template = \"\"\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRznzEwL3W-b"
   },
   "source": [
    "## **Step 3: Prepare the Dataset**\n",
    "\n",
    "This is a crucial step where we format our data into a structure the model can learn from. The process involves three parts:\n",
    "\n",
    "1.  **Loading**: We'll load the official competition dataset from Hugging Face.\n",
    "2.  **Splitting**: The full dataset is massive. For this starter notebook, we'll create a much smaller, more manageable version to speed things up: **5,000 samples for training** and **500 for validation**.\n",
    "3.  **Prompting**: We will format each data sample into a clear instructional prompt. This helps the model understand its role as a mathematician verifying a solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Load the full training dataset\n",
    "full_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
    "\n",
    "# Shuffle the dataset for randomness and create our smaller splits\n",
    "total_size = 40000\n",
    "total_size_buffered = int(total_size * 1.3)\n",
    "true_proportion = 0.50\n",
    "\n",
    "shuffled_dataset = full_dataset.shuffle(seed=44)\n",
    "train_dataset = shuffled_dataset.select(range(total_size_buffered))\n",
    "validation_dataset = shuffled_dataset.select(range(total_size_buffered, total_size_buffered + 1000))\n",
    "\n",
    "# added to ensure  true/false train split\n",
    "true_examples = train_dataset.filter(lambda x: x[\"is_correct\"] == True).select(range(int(total_size*true_proportion)))\n",
    "false_examples = train_dataset.filter(lambda x: x[\"is_correct\"] == False).select(range(int(total_size*(1-true_proportion))))\n",
    "train_dataset = concatenate_datasets([true_examples, false_examples]).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# This function formats our data samples using the Llama-3 chat template\n",
    "def formatting_prompts_func(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    solutions = examples[\"solution\"]\n",
    "    outputs = examples[\"is_correct\"] # These are True/False booleans\n",
    "    texts = []\n",
    "    for question, solution, output in zip(questions, solutions, outputs):\n",
    "        # Define the conversation messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a great mathematician. Your task is to verify if a given solution to a math problem is correct. Respond with only 'True' if the solution is correct, and only 'False' otherwise.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Question:\\n{question}\\n\\nSolution:\\n{str(solution)}\"},\n",
    "            {\"role\": \"assistant\", \"content\": str(output)}\n",
    "        ]\n",
    "\n",
    "        formatted_text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False, # We want the formatted string, not tokens yet\n",
    "            add_generation_prompt=False # We are providing the assistant's response\n",
    "        )\n",
    "        texts.append(formatted_text)\n",
    "\n",
    "    return { \"text\" : texts }\n",
    "\n",
    "# Apply the formatting function (this part stays the same)\n",
    "formatted_train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "formatted_validation_dataset = validation_dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(examples):\n",
    "    # Convert lists back to tensors and pad\n",
    "    ids  = [torch.tensor(e[\"input_ids\"], dtype=torch.long) for e in examples]\n",
    "    labs = [torch.tensor(e[\"labels\"],    dtype=torch.long) for e in examples]\n",
    "\n",
    "    ids  = torch.nn.utils.rnn.pad_sequence(\n",
    "        ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    labs = torch.nn.utils.rnn.pad_sequence(\n",
    "        labs, batch_first=True, padding_value=-100  # ignore_index for loss\n",
    "    )\n",
    "    attn = (ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "    return {\"input_ids\": ids, \"attention_mask\": attn, \"labels\": labs}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8Fs1qmn37-F"
   },
   "source": [
    "## **Step 4: Configure LoRA and Set Up the Trainer**\n",
    "\n",
    "### **LoRA Configuration**\n",
    "\n",
    "Instead of training the entire model (which has billions of parameters), we'll use a technique called **Lo**w-**R**ank **A**daptation (LoRA). üéõÔ∏è\n",
    "\n",
    "Think of it like this: rather than rewriting an entire textbook, we're just adding small, efficient \"sticky notes\" (the LoRA adapters) to update the model's knowledge. This is much faster and requires significantly less memory. We'll use a small **rank** (`r = 8`) to keep the training process light and quick for this starter notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 256, # A common practice is to set alpha = 2 * r\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCHdotc14DgH"
   },
   "source": [
    "\n",
    "### **SFTTrainer Setup**\n",
    "\n",
    "Now we'll set up the `SFTTrainer` (Supervised Fine-tuning Trainer). This is the main tool from the `trl` library that will handle the entire training loop for us. We'll give it our model, tokenizer, dataset, and a set of training instructions, such as the batch size and number of epochs.\n",
    "\n",
    "We will train for just **one epoch** (a single pass over our 5,000-sample dataset) to keep this demonstration fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = formatted_train_dataset,\n",
    "    eval_dataset = formatted_validation_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 64,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 10,\n",
    "        max_steps = 625,\n",
    "        learning_rate = 5e-5,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 300,\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 300,\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 500,\n",
    "        optim = \"adamw_bnb_8bit\", #\"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"/content/drive/MyDrive/llama_finetune_checkpoints\",\n",
    "        report_to = \"none\",\n",
    "\n",
    "        #gradient_checkpointing = True,             # reduces memory use\n",
    "        dataloader_num_workers = 12,                # increase for faster loading\n",
    "        group_by_length = True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nproc # if > 8, increase num_workers TO 16 IF POSSIBLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTHBzKeM4zF6"
   },
   "source": [
    "## **Step 5: Start Training\\!**\n",
    "\n",
    "Now, we'll call the `train()` function on our `trainer` object. This will kick off the fine-tuning process. Based on our settings, this will run for one full epoch over our 5,000 examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "#trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "save_path = \"/content/drive/MyDrive/unsloth_math_model\"\n",
    "\n",
    "# Save both base and LoRA\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"‚úÖ Model and tokenizer saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LfJfk5gIyIV"
   },
   "source": [
    "\n",
    "## **Step 6: Evaluation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Load the full training dataset\n",
    "full_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
    "test_dataset = full_dataset.shuffle(seed=45).select(range(1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Get token IDs for stopping\n",
    "eos_id = tokenizer.eos_token_id\n",
    "eot_id = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "predictions = []\n",
    "true_probs_list = []\n",
    "\n",
    "# Generate predictions for each test example\n",
    "for example in tqdm(test_dataset, desc=\"Generating predictions\"):\n",
    "    question = example[\"question\"]\n",
    "    solution = example[\"solution\"]\n",
    "\n",
    "    # Format using chat template (same as training/validation)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a great mathematician. Your task is to verify if a given solution to a math problem is correct. Respond with only 'True' if the solution is correct, and only 'False' otherwise.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nSolution:\\n{str(solution)}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate with proper stopping\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=5,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            num_beams=1,\n",
    "            eos_token_id=[eos_id, eot_id],\n",
    "            pad_token_id=eos_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    # Extract only newly generated tokens\n",
    "    prompt_len = inputs.shape[1]\n",
    "    gen_ids = outputs[0, prompt_len:]\n",
    "\n",
    "    # Cut at first EOT token\n",
    "    eot_positions = (gen_ids == eot_id).nonzero(as_tuple=True)[0]\n",
    "    if len(eot_positions) > 0:\n",
    "        gen_ids = gen_ids[:eot_positions[0].item()]\n",
    "\n",
    "    # Cut at first EOS token\n",
    "    eos_positions = (gen_ids == eos_id).nonzero(as_tuple=True)[0]\n",
    "    if len(eos_positions) > 0:\n",
    "        gen_ids = gen_ids[:eos_positions[0].item()]\n",
    "\n",
    "    # Decode and parse\n",
    "    response_text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Extract first word and normalize\n",
    "    first_word = response_text.split()[0] if response_text else \"\"\n",
    "\n",
    "    # Map to boolean (Kaggle expects True/False boolean values)\n",
    "    if first_word.lower().startswith(\"true\"):\n",
    "        prediction = True\n",
    "    elif first_word.lower().startswith(\"false\"):\n",
    "        prediction = False\n",
    "    else:\n",
    "        # Fallback: default to False if unparseable (shouldn't happen with good training)\n",
    "        prediction = False\n",
    "        print(f\"Warning: Unexpected output '{response_text}' - defaulting to False\")\n",
    "\n",
    "    predictions.append(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "y_pred = predictions\n",
    "y_true = list(test_dataset['is_correct'])\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Confusion:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ehz1Uly-JV-0"
   },
   "source": [
    "## **Step 7: Generate Submission File**\n",
    "\n",
    "This is the final step\\! We will now run our fine-tuned model on the official `test` dataset.\n",
    "\n",
    "We will loop through each example in the test set, generate a prediction, and format the results into a CSV file with two columns: `ID` and `is_correct`, as required by the competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 4096  # Or whatever you used\n",
    "dtype = None           # This will auto-detect\n",
    "load_in_4bit = True    # Use 4-bit quantization\n",
    "\n",
    "checkpoint_step = 8\n",
    "checkpoint_path = f\"/content/drive/MyDrive/llama_finetune_checkpoints/checkpoint-625\"\n",
    "print(f\"Loading model from: {checkpoint_path}\")\n",
    "\n",
    "# Load the fine-tuned model and tokenizer from your checkpoint\n",
    "model2, tokenizer2 = FastLanguageModel.from_pretrained(\n",
    "    model_name = checkpoint_path,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model2)\n",
    "\n",
    "# Load the official test set\n",
    "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
    "\n",
    "# Get token IDs for stopping\n",
    "eos_id = tokenizer2.eos_token_id\n",
    "eot_id = tokenizer2.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Generate predictions for each test example\n",
    "for example in tqdm(test_dataset, desc=\"Generating predictions\"):\n",
    "    question = example[\"question\"]\n",
    "    solution = example[\"solution\"]\n",
    "\n",
    "    # Format using chat template (same as training/validation)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a great mathematician. Your task is to verify if a given solution to a math problem is correct. Respond with only 'True' if the solution is correct, and only 'False' otherwise.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Question:\\n{question}\\n\\nSolution:\\n{str(solution)}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Apply chat template\n",
    "    inputs = tokenizer2.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate with proper stopping\n",
    "    with torch.no_grad():\n",
    "        outputs = model2.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=5,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            num_beams=1,\n",
    "            eos_token_id=[eos_id, eot_id],\n",
    "            pad_token_id=eos_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    # Extract only newly generated tokens\n",
    "    prompt_len = inputs.shape[1]\n",
    "    gen_ids = outputs[0, prompt_len:]\n",
    "\n",
    "    # Cut at first EOT token\n",
    "    eot_positions = (gen_ids == eot_id).nonzero(as_tuple=True)[0]\n",
    "    if len(eot_positions) > 0:\n",
    "        gen_ids = gen_ids[:eot_positions[0].item()]\n",
    "\n",
    "    # Cut at first EOS token\n",
    "    eos_positions = (gen_ids == eos_id).nonzero(as_tuple=True)[0]\n",
    "    if len(eos_positions) > 0:\n",
    "        gen_ids = gen_ids[:eos_positions[0].item()]\n",
    "\n",
    "    # Decode and parse\n",
    "    response_text = tokenizer2.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # Extract first word and normalize\n",
    "    first_word = response_text.split()[0] if response_text else \"\"\n",
    "\n",
    "    # Map to boolean (Kaggle expects True/False boolean values)\n",
    "    if first_word.lower().startswith(\"true\"):\n",
    "        prediction = True\n",
    "    elif first_word.lower().startswith(\"false\"):\n",
    "        prediction = False\n",
    "    else:\n",
    "        # Fallback: default to False if unparseable (shouldn't happen with good training)\n",
    "        prediction = False\n",
    "        print(f\"Warning: Unexpected output '{response_text}' - defaulting to False\")\n",
    "\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': range(len(predictions)),\n",
    "    'is_correct': predictions\n",
    "})\n",
    "checkpoint_step = 8750\n",
    "submission_path = f\"/content/drive/MyDrive/llama_finetune_checkpoints/submission-{checkpoint_step}.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Submission file 'submission.csv' created successfully!\")\n",
    "print(f\"Total predictions: {len(predictions)}\")\n",
    "print(f\"True: {sum(predictions)}, False: {len(predictions) - sum(predictions)}\")\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(submission.head(10))\n",
    "print(\"\\nYou can now download this file and submit it to the Kaggle competition.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.is_correct.value_counts()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
